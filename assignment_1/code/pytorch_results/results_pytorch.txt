Activation DNN_hidden_units "Learning Rate" Neg_Slope Batch_size Optimizer Accuracy Loss
"Leaky Relu" 100 0.002 0.01 200 Adam 0.5199 1.361
"Leaky Relu" 100 0.002 0.01 200 Adam 0.5185 1.354
"Leaky Relu" 200 0.002 0.01 200 Adam 0.5339 1.332
"Leaky Relu" 300 0.002 0.01 200 Adam 0.5297 1.335
"Leaky Relu" 200 0.003 0.01 200 Adam 0.5279 1.334
"Leaky Relu" 200 0.004 0.01 200 Adam 0.5165 1.354
"Leaky Relu" 200,200 0.003 0.01 200 Adam 0.5488 1.292
"Leaky Relu" 200,200,200 0.003 0.01 200 Adam 0.5502 1.282
"Leaky Relu" 200,100,200 0.003 0.01 200 Adam 0.5415 1.303
"Leaky Relu" 200,200,200,200 0.003 0.01 200 Adam 0.5425 1.303
"Leaky Relu" 300,200,100,200,300 0.003 0.01 200 Adam 0.5484 1.284
"Leaky Relu" 500,500,500 0.003 0.01 200 Adam 0.5509 1.269
"Leaky Relu" 200,200,200 0.003 0.01 300 Adam 0.5483 1.334
"Leaky Relu" 200,200,200 0.003 0.01 250 Adam 0.5487 1.322
"Leaky Relu" 200,200,200 0.003 0.01 150 Adam 0.5363 1.306
"Leaky Relu" 500,500,500 0.003 0.01 250 Adam 0.5583 1.334
"Leaky Relu" 500,500,500 0.003 0.02 250 Adam 0.5574 1.335
"Leaky Relu" 500,500,500 0.003 0.01 250 SDG 0.5013 1.415
"Leaky Relu" 500,500,500 0.003 0.01 250 Adam 0.5583 1.334
